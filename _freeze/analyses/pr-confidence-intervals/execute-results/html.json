{
  "hash": "295e0f0a4f3437d3bcf75e00de8d0749",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Confidence intervals for PR\"\nauthor: \"Lasse Hjorth Madsen\"\ndate: today \nformat: \n  html:\n    df-print: paged\n    fig-width: 4\n    fig-height: 4\nnumber-sections: false\ntoc: true\ntoc-expand: true\ntoc-depth: 3\neditor: source\nexecute:\n  echo: false\n  warning: false\n  cache: true\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n## Why this?\n\nPerformance Rating, PR, is the defacto standard for measuring playing strength in backgammon. It's the average error per decision (i.e. non-forced and non-trivial play). But for an individual player PR varies a lot from match to match, from tournament to tournament, and over time. This is expected, since players face very different sets of decisions in each match.\n\nA match is, in essence, a random sample of all possible decision one will have to make in backgammon. But random samples vary; some matches are hard, some not so hard. Let's try to examine that variation and to calculate confidence intervals around PR-values, to get an idea of how accurate a given PR-result is.\n\n## Examples\n\nSuppose I estimate my own playing strength at around, say, 5.0 PR. Now, let's say I play a 17-point match at 6.0 PR. What is the most reasonable interpretation:\n\n  - That I for some reason played worse than usual, or \n  - That the match was tougher that average?\n    \nFor another example, suppose that I play eight 11-point matches in some tournament, with a combined PR of 4.4. Then, a year later, I play 6.4 in a similar tournament. Is this evidence that my play has gotten worse, or is it as likely to be just statistical noise?\n\nThe last example was a real life experience for me at the qualification tournaments for the Danish national team in 2024 and 2025. It would be nice to more confidently interpret these data.\n\n## The data\n\nWe can use the [`bgmoves`](https://lassehjorthmadsen.github.io/bglab/reference/bgmoves.html) dataset to estimate the *variance* of my errors, which we need to compute confidence intervals around a specific PR result.\n\n`bgmoves` have 191012 positions, some involving both a checker play and a cube play. Not all positions in the database are decisions; if we pull out all actual decisions from the dataset, we get a subset of 163530 actual decisions.\n\nIn backgammon, the majority of checker play decisions are easy; both me and my opponents get the most checker plays right. Here's a count of checker play decisions, by player and if a an error was made: \n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Player\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Checker play error made\"],\"name\":[2],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"n\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Percent (by player)\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Lasse\",\"2\":\"FALSE\",\"3\":\"45843\",\"4\":\"75.2\"},{\"1\":\"Lasse\",\"2\":\"TRUE\",\"3\":\"15089\",\"4\":\"24.8\"},{\"1\":\"Someone else\",\"2\":\"FALSE\",\"3\":\"55721\",\"4\":\"70.7\"},{\"1\":\"Someone else\",\"2\":\"TRUE\",\"3\":\"23051\",\"4\":\"29.3\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nSo we got 70-75 percent of all checker play decisions right. One can guess that perhaps half of all checker plays are truly routine; among the other half some are pretty easy, some are too close to care about, and some are real problems. If I get half of the latter group right, that aligns roughly with the numbers above.\n\nCube decisions are different by nature, they are far fewer, but it looks like most are also relatively easy:\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Player\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Cube error made\"],\"name\":[2],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"n\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Percent (by player)\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Lasse\",\"2\":\"FALSE\",\"3\":\"7316\",\"4\":\"83.1\"},{\"1\":\"Lasse\",\"2\":\"TRUE\",\"3\":\"1488\",\"4\":\"16.9\"},{\"1\":\"Someone else\",\"2\":\"FALSE\",\"3\":\"11477\",\"4\":\"76.4\"},{\"1\":\"Someone else\",\"2\":\"TRUE\",\"3\":\"3545\",\"4\":\"23.6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n## PR estimates {#sec-estimates}\n\nWe really wanted to use the data to estimate PR and especially the uncertainty of those estimates, so let's get going.\n\nPR is the average error per decision, where decision is defined as non-obvious moves: Non-forced checker plays; checker plays where all moves are not equal; non-optional cubes; and potential doubles where doubling is a mistake less that 0.200 emg (the commonly used 'equivalent-to-money-game' metric).\n\nWhen we compute PR for a specific game or match or series of matches, it can be seen as an estimate of the true PR based on a random *sample* of all possible decision.\n\nLike this:\n\n\n::: {#tbl-out .cell .column-page-right tbl-cap='PR confidence intervals'}\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Player\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Decisions\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Mean error\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Standard deviation\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"PR\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Confidence interval\"],\"name\":[6],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"Lasse\",\"2\":\"69736\",\"3\":\"-0.011\",\"4\":\"0.038\",\"5\":\"5.259\",\"6\":\"± 0.142\"},{\"1\":\"Someone else\",\"2\":\"93794\",\"3\":\"-0.017\",\"4\":\"0.054\",\"5\":\"8.469\",\"6\":\"± 0.173\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nSo in this data set I average a PR of 5.259 ± 0.141. The calculation is based on a *lot* of decisions, 69,737. The mean error (in equivalent-to-money-game, emg) is -0.011, meaning that I on average sacrifice about a percentage of a point on each decision. That may seem low, but recall that it's across *all* decisions, also routine ones, and that most error are zero, so there has to be  occasionally really big mistakes in there. (Have a look at [this](https://lassehjorthmadsen.github.io/bganalyses/analyses/player-mistakes.html) analysis, for a more in-depth look at distributions of mistakes.)\n\nThe standard deviation is a measure of how much the errors vary around the mean; we see that the variation is a bit higher for my opponents. That makes sense, since this group includes quite a few inexperienced players, who makes bigger mistakes more frequently.\n\nPR is just the mean error multiplied by -500.\n\nThe [confidence interval, CI](https://en.wikipedia.org/wiki/Confidence_interval) is calculated as the standard deviation divided by the quare root of the number of observations, n:\n\n$$\n\\text{CI} = \\bar{x} \\pm z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}\n$$\nWhere:\n\n$$\n\\bar{x} \\text{ = sample mean}\n$$\n$$\nz_{\\alpha/2} \\text{ = critical value (1.96 for 95\\% confidence)}\n$$\n$$\n\\sigma \\text{ = standard deviation}\n$$\n$$\nn \\text{ = number of observations}\n$$\n\nWe can split up the calculations by decision type (checker or cube), and see that the variance is a bit higher for cube decision. The confidence intervals are quite a bit wider, since the number of decisions is smaller:\n\n\n::: {.cell .column-page-right}\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Player\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Decision type\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Decisions\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Mean error\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Standard deviation\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"PR\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Confidence interval\"],\"name\":[7],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"Lasse\",\"2\":\"Cube error\",\"3\":\"8804\",\"4\":\"-0.017\",\"5\":\"0.064\",\"6\":\"8.641\",\"7\":\"± 0.66\"},{\"1\":\"Lasse\",\"2\":\"Move error\",\"3\":\"60932\",\"4\":\"-0.010\",\"5\":\"0.033\",\"6\":\"4.770\",\"7\":\"± 0.13\"},{\"1\":\"Someone else\",\"2\":\"Cube error\",\"3\":\"15022\",\"4\":\"-0.028\",\"5\":\"0.086\",\"6\":\"13.991\",\"7\":\"± 0.69\"},{\"1\":\"Someone else\",\"2\":\"Move error\",\"3\":\"78772\",\"4\":\"-0.015\",\"5\":\"0.045\",\"6\":\"7.416\",\"7\":\"± 0.16\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n## Applications\n\n### A 17-point match\n\nIf, like in the examples mentioned in the beginning, you have a much smaller sample, like a single match, how will the calculations typically look like? Here's how many decisions we find, on average, per player, in matches of different length:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Length\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"No. of matches\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Mean no. of decisions\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Median no. of decisions\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"39\",\"3\":\"20.6\",\"4\":\"21.0\"},{\"1\":\"3\",\"2\":\"1651\",\"3\":\"43.1\",\"4\":\"40.0\"},{\"1\":\"5\",\"2\":\"669\",\"3\":\"67.6\",\"4\":\"65.0\"},{\"1\":\"7\",\"2\":\"110\",\"3\":\"98.0\",\"4\":\"95.0\"},{\"1\":\"9\",\"2\":\"12\",\"3\":\"101.0\",\"4\":\"105.0\"},{\"1\":\"11\",\"2\":\"2\",\"3\":\"194.0\",\"4\":\"194.0\"},{\"1\":\"13\",\"2\":\"96\",\"3\":\"185.0\",\"4\":\"178.5\"},{\"1\":\"17\",\"2\":\"16\",\"3\":\"227.8\",\"4\":\"223.0\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\nLet's say you play a 17-point match, with about 230 decisions on average. If your error variance is like my online error variance, then the confidence interval is about $\\pm2.5\\ PR$. That's a lot, meaning that a single 17-point match is usually not that informative.\n\nIf your error variance is more like my online opponents' error variance, then the confidence interval is even wider, about $\\pm3.5\\ PR$.\n\nIf, on your other hand, your are an exceptionally skilled player, like a friend of mine who plays under the nick 'Llabba', the variance is lower and the confidence interval tighter, about $\\pm1.6\\ PR$.\n\nAs in so many other cases, we can conclude that we generally need more data that we think.\n\n### A disastrous tournament\n\nLet's take a look at the second example from the beginning: Me playing 4.4 and 6.4 respectively, across eight 11-point matches in two PR-based tournaments.\n\nHow strong evidence is this, that I was weaker in the second tournament, rather that just getting particular hard problems, or being unlucky in the inevitable guesswork?\n\nKey numbers:\n\n   - First tournament: 1,244 decisions; PR 4.4\n   - Second tournament: 1,228 decisions; PR 6.4\n   - For both tournaments, we assume same standard deviation as in my online matches: 0.038\n\nWe can focus on the *difference* in PR between the two estimates by using the following variant of the formula above:\n\n$$\n\\text{CI} = \\bar{x}_1 - \\bar{x}_1 \\pm z_{\\alpha/2} \\cdot \\sqrt{  \\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_2^2}{n_2} }\n$$\nPlugging in numbers, remembering to multiply standard deviation, $\\sigma$, by 500 to get to PR. \n\n\n::: {.cell}\n\n:::\n\n\n$$\n\\text{CI} = 6.4 - 4.4 \\pm 1.96 \\cdot \\sqrt{\\frac{(500 \\cdot0.038)^2}{1228}+\\frac{(500 \\cdot0.038)^2}{1244}}) = 2 \\pm 1.5\n$$\nSince the confidence interval of the difference in PR of 2 does not include zero, it looks like there's some evidence that I played worse in the second tournament, and not just getting harder decisions.\n\nAnother way of looking at it is to express the difference in standard errors, or z-scores, which would be:\n\n$$\nz = \\frac{2}{\\frac{500 \\cdot0.038}{\\sqrt{1228}}+\\frac{500 \\cdot0.038}{\\sqrt{1244}}} = 2.61 \n$$\nWhich again translates to a p-value of 0.009.\n\nIf my backgammon skills really was the same in the two tournaments, I would observe a difference in PR of at least 2 with probability 0.009.\n\nOf course, the real explanation could be a combination: I might have had an harder job in the second tournament, and also was not playing my top game, perhaps due to distraction, fatigue, lack of motivation or whatever.\n\n### A wing fighter plot\n\nNow that we have the data, let's try to rank player by average PR in a nice plot.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell .column-page-right}\n::: {.cell-output-display}\n![](pr-confidence-intervals_files/figure-html/players_rank_plot-1.png){width=672}\n:::\n:::\n\n\nIn general, for random opponents on Galaxy Backgammon, there are too little data for the confidence intervals to be super meaningful. For myself and Llabba I have a lot more data so estimates are much more precise.\n\nNote, as mentioned earlier, that the confidence intervals depend not only on the number of decisions we have available, but also on the *standard deviation* of the errors, e.g. how consistent players are.\n\nNot surprisingly, strong players tend to have lower standard deviation as well; almost by definition they have fewer extreme values. We can see that in the plot below.\n\n\n::: {.cell .column-page-right}\n::: {.cell-output-display}\n![](pr-confidence-intervals_files/figure-html/players_sdt_dev-1.png){width=672}\n:::\n:::\n\n\n### A word of caution\n\nLet's take a quick moment to reiterate the concept of a confidence interval, since this is often misinterpreted. I'm using a customary confidence level of 95%. That means, that *before* playing a match, I know that my resulting PR will, with probability 95%, be within, say, $\\pm2.5$ of my true PR.\n\nIt does *not* mean that my true PR is within this interval from any result that I have already obtained. Once the match is finished, it either does or does not reflect my actual PR. The 95% refers to the *process* of collecting data from random sample, not the accuracy of any *specific* result.\n\n## Double checking\n\n### Calculate another way\n\nJust to check, let's re-calculate my confidence intervals from the table in @sec-estimates using a the `t.test()` function from the `stats` package in R -- as standard as it gets.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmy_errors <- df |> \n  filter(Player == \"Lasse\") |> \n  pull(error_size) |> \n  (`*`)(-500)\n\ntest_result <- my_errors |> t.test()\n\nmy_mean <- test_result$estimate |> round(3)\nmy_ci <- (test_result$estimate - test_result$conf.int[1]) |> round(3)\n\ncat(\"Mean PR for player Lasse            : \", my_mean, \"\\r\",\n    \"Confidence interval for player Lasse: \", my_ci,\n    sep = \"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMean PR for player Lasse            : 5.259\nConfidence interval for player Lasse: 0.141\n```\n\n\n:::\n:::\n\nThis agrees quite closely; the small discrepancy is likely due to using the t-distribution which in this case is *nearly* identical to the normal distribution.\n\n### Normally distributed?\n\nIt would be nice to check if the data actually behaves as assumed above: That PR estimates follows a normal distribution around the true mean, with the standard deviation of the estimates declining by the square root of the number of observations. (This should hold even though the distribution of errors is far from normal, as stated by the [Central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem)).\n\nTo check, we can treat each individual game in the dataset as a sample, and see if game-level average errors are indeed distributed normally around players' overall error rate. We expect games with fewer decisions to show larger variability than games with many decisions, but they should be approximately normal distributed around the 'true' mean.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](pr-confidence-intervals_files/figure-html/z-score-plot-1.png){width=384}\n:::\n:::\n\n\nActually, the distribution does not quite follow the symmetrical shape of the normal distribution. In hindsight this is not so surprising: The PR value cannot go *below* zero but the upper bound is very high. In other words: There's a hard limit to how well you can play in a given game (zero PR), but the potential to screw up is huge. You can be a large number of standard deviations *above* your average, but only so much *lower*.\n\n### Bootstrap\n\nTo check if this affect the confidence intervals calculated above, let's try [boot strapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) -- a numerical approach that doesn't rely on assumptions of the underlying distribution or the [Central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem)).\n\nAgain, we compare to my own confidence interval from @sec-estimates:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nBoot strapped mean PR for player Lasse: 5.26\nBoot strapped CI for player Lasse     : 0.139\n```\n\n\n:::\n:::\n\n\nAgain very close; small differences can be due to the stochastical nature of boot strapping.\n\nWe can conclude that the confidence intervals presented earlier are likely to be valid.\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}